import ss
import socket
import whois
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import pyfiglet

def print_title():
    title = pyfiglet.figlet_formaStep 1: Crawl the website
def crawl_website(base_url):
    urls_to_crawl = [base_url]
    crawled_urls = set()

    while urls_to_crawl:
        url = urls_to_crawl.pop()
        if url in crawled_urls:
            continue
        
        try:
         nd_all('a'):
                href = link.get('href')
                full_url = urljoin(base_url, href)
                if full_url not in crawled_urls:
                    urls_to_crawl.append(full_url)

        except requests.RequestEx            print(f"Failed to crawl {url}: {e}")

    return crawled_urls
# Step 2: Check SSL/TLS validity
def check_ssl_tls(domain):
    context = ssl.create_default_context()
    conn = context.wrap_socket(socket.socket(socket.AF_INET), server_hostname=domain)
    conn.settimeout(5.0)

    try:
        conn.connect((domain, 443))
        cert = conn.getpeercert()
        return True, cert
    except Exception as e:
        return False, str(e)

# Step 3: Identify website hosting provider
def get_hosting_provider(domain):
    try:
        domain_info = whois.whois(domain)
        return domain_info.org
    except Exception as e:
        return str(e)

# Step 4: Chefor security headers
def check_security_headers(url):
    security_headers = [
        "Content-Security-Policy", "Strict-Transport-Security", "X-Content-Type-Options",
        "X-Frame-Options", "X-XSS-Protection"
    ]
    headers_info = {}

    try:
        response = requests.get(url)
        for header in security_headers:
            headers_info[header] = response.headers.get(header, "Not Found")
    except requests.RequestException as e:
        headers_info["error"] = str(e)

    return headers_info

# Step 5: Check for SQL injection and XSS vulnerabilities
def check_sql_injection(urls):
    sql_payload = "' OR '1'='1"
    vulnerable_urls = []

    for url in urls:
        try:
            response = requests.get(f"{url}{sql_payload}")
            if "error" not in response.text.lower():  # Simplistic check
                vulnerable_urls.append(url)
        except requests.RequestException as e:
            print(f"Failed to test {url}: {e}")

    return vulnerable_urls

def check_xss(urls):
    xss_payload = "<script>alert('XSS')</script>"
    vulnerable_urls = []

    for url in urls:
        try:
            response = requests.get(f"{url}{xss_payload}")
            if xss_payload in response.text:  # Simplistic check
                vulnerable_urls.append(url)
        except requests.RequestException as e:
            print(f"Failed to test {url}: {e}")

    return vulnerable_urls

# Step 6: Check for directory listing vulnerabilities
def check_directory_listing(urls):
    vulnerable_urls = []

    for url in urls:
        try:
            response = requests.get(url)
            if "Index of /" in response.text:
                vulnerable_urls.append(url)
        except requests.RequestException as e:
            print(f"Failed to test {url}: {e}")

    return vulnerable_urls

# Step 7: Check server information
def check_outdated_server(url):
    try:
        response = requests.get(url)
        server_header = response.headers.get("Server", "Not Found")
        return server_header
    except requests.RequestException as e:
        return str(e)

# Step 8: Generate a report
def generate_report(domain, ssl_info, hosting_provider, security_headers, sql_vulnerable_urls, xss_vulnerable_urls, dir_vulnerable_urls, server_info):
    report = f"Website Scan Report for {domain}\n\n"
    report += f"SSL/TLS Valid: {ssl_info[0]}, Info: {ssl_info[1]}\n"
    report += f"Hosting Provider: {hosting_provider}\n\n"
    report += "Security Headers:\n"
    for header, value in security_headers.items():
        report += f"{header}: {value}\n"
    
    report += "\nSQL Injection Vulnerable URLs:\n"
    report += "\n".join(sql_vulnerable_urls) + "\n"

    report += "\nXSS Vulnerable URLs:\n"
    report += "\n".join(xss_vulnerable_urls) + "\n"

    report += "\nDirectory Listing Vulnerable URLs:\n"
    report += "\n".join(dir_vulnerable_urls) + "\n"

    report += f"\nServer Info: {server_info}\n"

    with open(f"{domain}_scan_report.txt", "w") as report_file:
        report_file.write(report)

# Main function to run all checks
def main(domain):
    base_url = f"http://{domain}"
    crawled_urls = crawl_website(base_url)

    ssl_info = check_ssl_tls(domain)
    hosting_provider = get_hosting_provider(domain)
    security_headers = check_security_headers(base_url)
    sql_vulnerable_urls = check_sql_injection(crawled_urls)
    xss_vulnerable_urls = check_xss(crawled_urls)
    dir_vulnerable_urls = check_directory_listing(crawled_urls)
    server_info = check_outdated_server(base_url)

    generate_report(domain, ssl_info, hosting_provider, security_headers, sql_vulnerable_urls, xss_vulnerable_urls, dir_vulnerable_urls, server_info)
    print(f"Scan report generated: {domain}_scan_report.txt")

if __name__ == "__main__":
    print_title()
    domain = input("Enter the domain to scan: ")
    main(domain)

